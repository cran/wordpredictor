<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />

<meta name="viewport" content="width=device-width, initial-scale=1" />



<title>Overview</title>

<script>// Pandoc 2.9 adds attributes on both header and div. We remove the former (to
// be compatible with the behavior of Pandoc < 2.8).
document.addEventListener('DOMContentLoaded', function(e) {
  var hs = document.querySelectorAll("div.section[class*='level'] > :first-child");
  var i, h, a;
  for (i = 0; i < hs.length; i++) {
    h = hs[i];
    if (!/^h[1-6]$/i.test(h.tagName)) continue;  // it should be a header h1-h6
    a = h.attributes;
    while (a.length > 0) h.removeAttribute(a[0].name);
  }
});
</script>

<style type="text/css">
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
span.underline{text-decoration: underline;}
div.column{display: inline-block; vertical-align: top; width: 50%;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
</style>



<style type="text/css">
code {
white-space: pre;
}
.sourceCode {
overflow: visible;
}
</style>
<style type="text/css" data-origin="pandoc">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
{ counter-reset: source-line 0; }
pre.numberSource code > span
{ position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
{ content: counter(source-line);
position: relative; left: -1em; text-align: right; vertical-align: baseline;
border: none; display: inline-block;
-webkit-touch-callout: none; -webkit-user-select: none;
-khtml-user-select: none; -moz-user-select: none;
-ms-user-select: none; user-select: none;
padding: 0 4px; width: 4em;
color: #aaaaaa;
}
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa; padding-left: 4px; }
div.sourceCode
{ }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } 
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } 
code span.at { color: #7d9029; } 
code span.bn { color: #40a070; } 
code span.bu { color: #008000; } 
code span.cf { color: #007020; font-weight: bold; } 
code span.ch { color: #4070a0; } 
code span.cn { color: #880000; } 
code span.co { color: #60a0b0; font-style: italic; } 
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } 
code span.do { color: #ba2121; font-style: italic; } 
code span.dt { color: #902000; } 
code span.dv { color: #40a070; } 
code span.er { color: #ff0000; font-weight: bold; } 
code span.ex { } 
code span.fl { color: #40a070; } 
code span.fu { color: #06287e; } 
code span.im { color: #008000; font-weight: bold; } 
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } 
code span.kw { color: #007020; font-weight: bold; } 
code span.op { color: #666666; } 
code span.ot { color: #007020; } 
code span.pp { color: #bc7a00; } 
code span.sc { color: #4070a0; } 
code span.ss { color: #bb6688; } 
code span.st { color: #4070a0; } 
code span.va { color: #19177c; } 
code span.vs { color: #4070a0; } 
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } 
</style>
<script>
// apply pandoc div.sourceCode style to pre.sourceCode instead
(function() {
  var sheets = document.styleSheets;
  for (var i = 0; i < sheets.length; i++) {
    if (sheets[i].ownerNode.dataset["origin"] !== "pandoc") continue;
    try { var rules = sheets[i].cssRules; } catch (e) { continue; }
    var j = 0;
    while (j < rules.length) {
      var rule = rules[j];
      // check if there is a div.sourceCode rule
      if (rule.type !== rule.STYLE_RULE || rule.selectorText !== "div.sourceCode") {
        j++;
        continue;
      }
      var style = rule.style.cssText;
      // check if color or background-color is set
      if (rule.style.color === '' && rule.style.backgroundColor === '') {
        j++;
        continue;
      }
      // replace div.sourceCode by a pre.sourceCode rule
      sheets[i].deleteRule(j);
      sheets[i].insertRule('pre.sourceCode{' + style + '}', j);
    }
  }
})();
</script>



<style type="text/css">

div.csl-bib-body { }
div.csl-entry {
clear: both;
margin-bottom: 0em;
}
.hanging div.csl-entry {
margin-left:2em;
text-indent:-2em;
}
div.csl-left-margin {
min-width:2em;
float:left;
}
div.csl-right-inline {
margin-left:2em;
padding-left:1em;
}
div.csl-indent {
margin-left: 2em;
}
</style>

<style type="text/css">body {
background-color: #fff;
margin: 1em auto;
max-width: 700px;
overflow: visible;
padding-left: 2em;
padding-right: 2em;
font-family: "Open Sans", "Helvetica Neue", Helvetica, Arial, sans-serif;
font-size: 14px;
line-height: 1.35;
}
#TOC {
clear: both;
margin: 0 0 10px 10px;
padding: 4px;
width: 400px;
border: 1px solid #CCCCCC;
border-radius: 5px;
background-color: #f6f6f6;
font-size: 13px;
line-height: 1.3;
}
#TOC .toctitle {
font-weight: bold;
font-size: 15px;
margin-left: 5px;
}
#TOC ul {
padding-left: 40px;
margin-left: -1.5em;
margin-top: 5px;
margin-bottom: 5px;
}
#TOC ul ul {
margin-left: -2em;
}
#TOC li {
line-height: 16px;
}
table {
margin: 1em auto;
border-width: 1px;
border-color: #DDDDDD;
border-style: outset;
border-collapse: collapse;
}
table th {
border-width: 2px;
padding: 5px;
border-style: inset;
}
table td {
border-width: 1px;
border-style: inset;
line-height: 18px;
padding: 5px 5px;
}
table, table th, table td {
border-left-style: none;
border-right-style: none;
}
table thead, table tr.even {
background-color: #f7f7f7;
}
p {
margin: 0.5em 0;
}
blockquote {
background-color: #f6f6f6;
padding: 0.25em 0.75em;
}
hr {
border-style: solid;
border: none;
border-top: 1px solid #777;
margin: 28px 0;
}
dl {
margin-left: 0;
}
dl dd {
margin-bottom: 13px;
margin-left: 13px;
}
dl dt {
font-weight: bold;
}
ul {
margin-top: 0;
}
ul li {
list-style: circle outside;
}
ul ul {
margin-bottom: 0;
}
pre, code {
background-color: #f7f7f7;
border-radius: 3px;
color: #333;
white-space: pre-wrap; 
}
pre {
border-radius: 3px;
margin: 5px 0px 10px 0px;
padding: 10px;
}
pre:not([class]) {
background-color: #f7f7f7;
}
code {
font-family: Consolas, Monaco, 'Courier New', monospace;
font-size: 85%;
}
p > code, li > code {
padding: 2px 0px;
}
div.figure {
text-align: center;
}
img {
background-color: #FFFFFF;
padding: 2px;
border: 1px solid #DDDDDD;
border-radius: 3px;
border: 1px solid #CCCCCC;
margin: 0 5px;
}
h1 {
margin-top: 0;
font-size: 35px;
line-height: 40px;
}
h2 {
border-bottom: 4px solid #f7f7f7;
padding-top: 10px;
padding-bottom: 2px;
font-size: 145%;
}
h3 {
border-bottom: 2px solid #f7f7f7;
padding-top: 10px;
font-size: 120%;
}
h4 {
border-bottom: 1px solid #f7f7f7;
margin-left: 8px;
font-size: 105%;
}
h5, h6 {
border-bottom: 1px solid #ccc;
font-size: 105%;
}
a {
color: #0033dd;
text-decoration: none;
}
a:hover {
color: #6666ff; }
a:visited {
color: #800080; }
a:visited:hover {
color: #BB00BB; }
a[href^="http:"] {
text-decoration: underline; }
a[href^="https:"] {
text-decoration: underline; }

code > span.kw { color: #555; font-weight: bold; } 
code > span.dt { color: #902000; } 
code > span.dv { color: #40a070; } 
code > span.bn { color: #d14; } 
code > span.fl { color: #d14; } 
code > span.ch { color: #d14; } 
code > span.st { color: #d14; } 
code > span.co { color: #888888; font-style: italic; } 
code > span.ot { color: #007020; } 
code > span.al { color: #ff0000; font-weight: bold; } 
code > span.fu { color: #900; font-weight: bold; } 
code > span.er { color: #a61717; background-color: #e3d2d2; } 
</style>




</head>

<body>




<h1 class="title toc-ignore">Overview</h1>



<div id="introduction" class="section level2">
<h2>Introduction</h2>
<p>This document describes the theory behind the n-gram models generated
by the <strong>wordpredictor</strong> package. It also provides code
examples that describe how to use the package.</p>
<p>The goal of the <strong>wordpredictor</strong> package is to provide
a flexible and easy to use framework for generating <a href="https://en.wikipedia.org/wiki/N-gram">n-gram models</a> for word
prediction.</p>
<p>The package allows generating n-gram models. It also allows exploring
n-gram frequencies using plots. Additionally it provides methods for
measuring n-gram model performance using <a href="https://en.wikipedia.org/wiki/Perplexity">Perplexity</a> and
accuracy.</p>
<p>The n-gram model may be customized using several options such as
n-gram size, data cleaning options and options for converting text to
tokens.</p>
</div>
<div id="how-the-model-works" class="section level2">
<h2>How the model works</h2>
<p>The n-gram model generated by the <strong>wordpredictor</strong>
package uses the <a href="https://en.wikipedia.org/wiki/Markov_chain">Markov model</a> for
approximating the language model. It means that the probability of a
word depends only on the probability of the n-1 previous words.</p>
<p>Maximum Likelihood Estimation (MLE) is used to calculate the
probability of a word. The probability of a word is calculated by
regarding the word as the last component of a n-gram. The total number
of occurrences of the n-gram is divided by the total number of
occurrences of the (n-1)-gram. This gives the probability for the
word.</p>
<p>The n-gram model is generated in steps. In the first step, the input
data is cleaned. Unwanted symbols and words are removed from the input
data.</p>
<p>In the next step, the cleaned data file is read. N-grams are
extracted from the file, starting from 1-grams up to the configured
n-gram size. The 1-gram, 2-gram, 3-gram etc tokens are saved in separate
files along with the frequency. So the 3-gram file contains all
extracted 3-grams and their respective frequencies.</p>
<p>The next step is to generate transition probability tables for each
n-gram file. For the 1-gram file the transition probability table is
simply the list of unique words along with the word frequencies. For the
other n-gram files, the transition probability table is a data frame
with 3 columns. The hash of n-gram prefixes, the next word id and the
next word probability.</p>
<p>The n-gram prefix is the set of n-1 components before the last
component. The n-1 components are combined using “_” and converted to a
numeric hash value using the digest2Int method of the <a href="https://cran.r-project.org/package=digest">digest</a> package.</p>
<p>The next word id is the numeric index of the next word in the list of
1-grams. The next word probability is the probability of the next word
given the previous n-1 words. It is calculated using Maximum Likelihood
Estimation (MLE) as described above.</p>
<p>Instead of storing the n-gram prefix strings, a single number is
saved. Also instead of storing the next word, the numeric index of the
next word is saved. This saves a lot of memory and allows more data to
be stored, which improves the n-gram model’s efficiency.</p>
<p>In R, a number requires a fixed amount of storage, which about 56
bytes. In contrast the memory required to store a string increases with
the number of characters in the string.</p>
<p>The data frames that represent each transition probability table are
combined into a single data frame. The combined transition probability
table is used to make word predictions.</p>
</div>
<div id="using-the-model-to-predict-words" class="section level2">
<h2>Using the model to predict words</h2>
<p>To predict a word, the word along with the n-1 previous words are
used as input. The model computes the hash of the previous words and
looks up the hash in the combined transition probabilities table. If the
hash was found, then the model extracts the top 3 next word ids that
have the highest probabilities.</p>
<p>The model looks up the next word text that corresponds to the next
word ids. The result is the top 3 most likely next words along with
their probabilities.</p>
<p>If the hash was not found, then the hash of the n-2 previous words is
calculated and looked up in the combined transition probabilities
table.</p>
<p>This process is repeated until there are no previous words. When this
happens, the model returns a “word not found” message.</p>
<p>This method of checking the transition probabilities of lower level
n-grams is called <strong>back-off</strong>. An alternate method of
predicting a word is to use <strong>interpolation</strong>. This
involves weighing and summing the probabilities for each n-gram
size.</p>
</div>
<div id="predicting-the-model-performance" class="section level2">
<h2>Predicting the model performance</h2>
<p>The <strong>wordpredictor</strong> package provides methods for
performing <strong>intrinsic</strong> and <strong>extrinsic</strong>
evaluation of the n-gram model.</p>
<p>The <strong>wordpredictor</strong> package performs intrinsic
evaluation by calculating the mean Perplexity score for all sentences in
a validation text file. The Perplexity for a sentence is calculated by
taking the N-th root of the inverse of the product of probabilities of
all words in a sentence. N is the number of words in the sentence.</p>
<p>The probability of a word is calculated by considering all n-1 words
before that word. If the word was not found in the transition
probabilities table, then the n-2 words are looked up. This process is
repeated until there are no previous words.</p>
<p>If the word was found in the 1-gram list, then the probability of the
word is calculated by simply dividing the number of times the word
occurs by the total number words.</p>
<p>If the word was not found in the 1-gram list, then the model uses a
default probability as the probability of the word. The default
probability is calculated using Laplace Smoothing.</p>
<p>Laplace Smoothing involves adding 1 to the frequency count of each
word in the vocabulary. Essentially this means that the total number of
words in the data set are increased by vc, where vc is the number of
words in the vocabulary.</p>
<p>In Laplace Smoothing 1 is added to the word count. Since an unknown
word occurs zero times, after Laplace Smoothing it will have a count of
1. So the default probability is calculated as: <strong>P(unk) =
1/(N+VC)</strong>, where <strong>N</strong> is the total number of words
in the data set and <strong>VC</strong> is the number of words in the
vocabulary. This default probability is assigned to unknown words.
Alternative methods to Laplace Smoothing are <strong>Add-k
smoothing</strong>, <strong>Kneser-Ney smoothing</strong> and
<strong>Good-Turing Smoothing</strong>.</p>
<p>The <strong>wordpredictor</strong> package uses the file
<strong>/usr/share/dict/cracklib-small</strong> as the dictionary file.
This file is pre-installed in most Linux distributions.</p>
<p>Extrinsic evaluation involves calculating the accuracy score. The
model tries to predict the last word of a sentence. If the actual last
word was one of the 3 words predicted by the model, then the prediction
is considered to be accurate. The accuracy score is the number of
sentences that were correctly predicted.</p>
</div>
<div id="environment-setup-code" class="section level2">
<h2>Environment setup code</h2>
<p>The following code should be run before running the examples.</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a><span class="fu">library</span>(wordpredictor)</span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a></span>
<span id="cb1-3"><a href="#cb1-3" tabindex="-1"></a><span class="co"># The level of verbosity in the information messages</span></span>
<span id="cb1-4"><a href="#cb1-4" tabindex="-1"></a>ve <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb1-5"><a href="#cb1-5" tabindex="-1"></a></span>
<span id="cb1-6"><a href="#cb1-6" tabindex="-1"></a><span class="co">#&#39; @description</span></span>
<span id="cb1-7"><a href="#cb1-7" tabindex="-1"></a><span class="co">#&#39; Used to setup the test environment</span></span>
<span id="cb1-8"><a href="#cb1-8" tabindex="-1"></a><span class="co">#&#39; @param rf The required files.</span></span>
<span id="cb1-9"><a href="#cb1-9" tabindex="-1"></a><span class="co">#&#39; @param ve The verbosity level.</span></span>
<span id="cb1-10"><a href="#cb1-10" tabindex="-1"></a><span class="co">#&#39; @return The list of directories in the test environment</span></span>
<span id="cb1-11"><a href="#cb1-11" tabindex="-1"></a>setup_env <span class="ot">&lt;-</span> <span class="cf">function</span>(rf, ve) {</span>
<span id="cb1-12"><a href="#cb1-12" tabindex="-1"></a>    <span class="co"># An object of class EnvManager is created</span></span>
<span id="cb1-13"><a href="#cb1-13" tabindex="-1"></a>    em <span class="ot">&lt;-</span> EnvManager<span class="sc">$</span><span class="fu">new</span>(<span class="at">rp =</span> <span class="st">&quot;../&quot;</span>, <span class="at">ve =</span> ve)</span>
<span id="cb1-14"><a href="#cb1-14" tabindex="-1"></a>    <span class="co"># The required files are downloaded</span></span>
<span id="cb1-15"><a href="#cb1-15" tabindex="-1"></a>    ed <span class="ot">&lt;-</span> em<span class="sc">$</span><span class="fu">setup_env</span>(rf)</span>
<span id="cb1-16"><a href="#cb1-16" tabindex="-1"></a></span>
<span id="cb1-17"><a href="#cb1-17" tabindex="-1"></a>    <span class="fu">return</span>(ed)</span>
<span id="cb1-18"><a href="#cb1-18" tabindex="-1"></a>}</span>
<span id="cb1-19"><a href="#cb1-19" tabindex="-1"></a></span>
<span id="cb1-20"><a href="#cb1-20" tabindex="-1"></a><span class="co">#&#39; @description</span></span>
<span id="cb1-21"><a href="#cb1-21" tabindex="-1"></a><span class="co">#&#39; Used to clean up the test environment</span></span>
<span id="cb1-22"><a href="#cb1-22" tabindex="-1"></a>clean_up <span class="ot">&lt;-</span> <span class="cf">function</span>(ve) {</span>
<span id="cb1-23"><a href="#cb1-23" tabindex="-1"></a>    <span class="co"># An object of class EnvManager is created</span></span>
<span id="cb1-24"><a href="#cb1-24" tabindex="-1"></a>    em <span class="ot">&lt;-</span> EnvManager<span class="sc">$</span><span class="fu">new</span>(<span class="at">ve =</span> ve)</span>
<span id="cb1-25"><a href="#cb1-25" tabindex="-1"></a>    <span class="co"># The test environment is removed</span></span>
<span id="cb1-26"><a href="#cb1-26" tabindex="-1"></a>    em<span class="sc">$</span><span class="fu">td_env</span>(F)</span>
<span id="cb1-27"><a href="#cb1-27" tabindex="-1"></a>}</span></code></pre></div>
</div>
<div id="generating-the-model" class="section level2">
<h2>Generating the model</h2>
<p>The <strong>ModelGenerator</strong> class allows generating the final
n-gram model using a single method call. The following example generates
a n-gram model using default data cleaning and tokenization options:</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" tabindex="-1"></a><span class="co"># The required files</span></span>
<span id="cb2-2"><a href="#cb2-2" tabindex="-1"></a>rf <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;input.txt&quot;</span>)</span>
<span id="cb2-3"><a href="#cb2-3" tabindex="-1"></a><span class="co"># The test environment is setup</span></span>
<span id="cb2-4"><a href="#cb2-4" tabindex="-1"></a>ed <span class="ot">&lt;-</span> <span class="fu">setup_env</span>(rf, ve)</span>
<span id="cb2-5"><a href="#cb2-5" tabindex="-1"></a></span>
<span id="cb2-6"><a href="#cb2-6" tabindex="-1"></a><span class="co"># The following code generates n-gram model using default options for data</span></span>
<span id="cb2-7"><a href="#cb2-7" tabindex="-1"></a><span class="co"># cleaning and tokenization. See the following section on how to customize these</span></span>
<span id="cb2-8"><a href="#cb2-8" tabindex="-1"></a><span class="co"># options. Note that input.txt is the name of the input data file. It should be</span></span>
<span id="cb2-9"><a href="#cb2-9" tabindex="-1"></a><span class="co"># present in the ed directory. The generated model file is also placed in this</span></span>
<span id="cb2-10"><a href="#cb2-10" tabindex="-1"></a><span class="co"># directory.</span></span>
<span id="cb2-11"><a href="#cb2-11" tabindex="-1"></a></span>
<span id="cb2-12"><a href="#cb2-12" tabindex="-1"></a><span class="co"># ModelGenerator class object is created</span></span>
<span id="cb2-13"><a href="#cb2-13" tabindex="-1"></a>mg <span class="ot">&lt;-</span> ModelGenerator<span class="sc">$</span><span class="fu">new</span>(</span>
<span id="cb2-14"><a href="#cb2-14" tabindex="-1"></a>    <span class="at">name =</span> <span class="st">&quot;def-model&quot;</span>,</span>
<span id="cb2-15"><a href="#cb2-15" tabindex="-1"></a>    <span class="at">desc =</span> <span class="st">&quot;N-gram model generating using default options&quot;</span>,</span>
<span id="cb2-16"><a href="#cb2-16" tabindex="-1"></a>    <span class="at">fn =</span> <span class="st">&quot;def-model.RDS&quot;</span>,</span>
<span id="cb2-17"><a href="#cb2-17" tabindex="-1"></a>    <span class="at">df =</span> <span class="st">&quot;input.txt&quot;</span>,</span>
<span id="cb2-18"><a href="#cb2-18" tabindex="-1"></a>    <span class="at">n =</span> <span class="dv">4</span>,</span>
<span id="cb2-19"><a href="#cb2-19" tabindex="-1"></a>    <span class="at">ssize =</span> <span class="fl">0.1</span>,</span>
<span id="cb2-20"><a href="#cb2-20" tabindex="-1"></a>    <span class="at">dir =</span> ed,</span>
<span id="cb2-21"><a href="#cb2-21" tabindex="-1"></a>    <span class="at">dc_opts =</span> <span class="fu">list</span>(),</span>
<span id="cb2-22"><a href="#cb2-22" tabindex="-1"></a>    <span class="at">tg_opts =</span> <span class="fu">list</span>(),</span>
<span id="cb2-23"><a href="#cb2-23" tabindex="-1"></a>    <span class="at">ve =</span> ve</span>
<span id="cb2-24"><a href="#cb2-24" tabindex="-1"></a>)</span>
<span id="cb2-25"><a href="#cb2-25" tabindex="-1"></a></span>
<span id="cb2-26"><a href="#cb2-26" tabindex="-1"></a><span class="co"># Generates n-gram model. The output is the file</span></span>
<span id="cb2-27"><a href="#cb2-27" tabindex="-1"></a><span class="co"># ./data/model/def-model.RDS</span></span>
<span id="cb2-28"><a href="#cb2-28" tabindex="-1"></a>mg<span class="sc">$</span><span class="fu">generate_model</span>()</span>
<span id="cb2-29"><a href="#cb2-29" tabindex="-1"></a></span>
<span id="cb2-30"><a href="#cb2-30" tabindex="-1"></a><span class="co"># The test environment is cleaned up</span></span>
<span id="cb2-31"><a href="#cb2-31" tabindex="-1"></a><span class="fu">clean_up</span>(ve)</span></code></pre></div>
</div>
<div id="evaluating-the-model-performance" class="section level2">
<h2>Evaluating the model performance</h2>
<p>The <strong>wordpredictor</strong> package provides the
<strong>ModelEvaluator</strong> class for evaluating the performance of
the generated n-gram model.</p>
<p>The following example performs intrinsic evaluation. It measures the
Perplexity score for each sentence in the
<strong>validation.txt</strong> file. It returns the minimum, mean and
maximum Perplexity score for each line.</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" tabindex="-1"></a><span class="co"># The required files</span></span>
<span id="cb3-2"><a href="#cb3-2" tabindex="-1"></a>rf <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;def-model.RDS&quot;</span>, <span class="st">&quot;validate-clean.txt&quot;</span>)</span>
<span id="cb3-3"><a href="#cb3-3" tabindex="-1"></a><span class="co"># The test environment is setup</span></span>
<span id="cb3-4"><a href="#cb3-4" tabindex="-1"></a>ed <span class="ot">&lt;-</span> <span class="fu">setup_env</span>(rf, ve)</span>
<span id="cb3-5"><a href="#cb3-5" tabindex="-1"></a></span>
<span id="cb3-6"><a href="#cb3-6" tabindex="-1"></a><span class="co"># The model file name</span></span>
<span id="cb3-7"><a href="#cb3-7" tabindex="-1"></a>mfn <span class="ot">&lt;-</span> <span class="fu">paste0</span>(ed, <span class="st">&quot;/def-model.RDS&quot;</span>)</span>
<span id="cb3-8"><a href="#cb3-8" tabindex="-1"></a><span class="co"># The path to the cleaned validation file</span></span>
<span id="cb3-9"><a href="#cb3-9" tabindex="-1"></a>vfn <span class="ot">&lt;-</span> <span class="fu">paste0</span>(ed, <span class="st">&quot;/validate-clean.txt&quot;</span>)</span>
<span id="cb3-10"><a href="#cb3-10" tabindex="-1"></a><span class="co"># ModelEvaluator class object is created</span></span>
<span id="cb3-11"><a href="#cb3-11" tabindex="-1"></a>me <span class="ot">&lt;-</span> ModelEvaluator<span class="sc">$</span><span class="fu">new</span>(<span class="at">mf =</span> mfn, <span class="at">ve =</span> ve)</span>
<span id="cb3-12"><a href="#cb3-12" tabindex="-1"></a><span class="co"># The intrinsic evaluation is performed on first 20 lines</span></span>
<span id="cb3-13"><a href="#cb3-13" tabindex="-1"></a>stats <span class="ot">&lt;-</span> me<span class="sc">$</span><span class="fu">intrinsic_evaluation</span>(<span class="at">lc =</span> <span class="dv">20</span>, <span class="at">fn =</span> vfn)</span>
<span id="cb3-14"><a href="#cb3-14" tabindex="-1"></a></span>
<span id="cb3-15"><a href="#cb3-15" tabindex="-1"></a><span class="co"># The test environment is cleaned up</span></span>
<span id="cb3-16"><a href="#cb3-16" tabindex="-1"></a><span class="fu">clean_up</span>(ve)</span></code></pre></div>
<p>The following example performs extrinsic evaluation. It measures the
accuracy score for each sentence in <strong>validation.txt</strong>
file. For each sentence the model is used to predict the last word in
the sentence given the previous words. If the last word was correctly
predicted, then the prediction is considered to be accurate. The
extrinsic evaluation returns the number of correct and incorrect
predictions.</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" tabindex="-1"></a><span class="co"># The required files</span></span>
<span id="cb4-2"><a href="#cb4-2" tabindex="-1"></a>rf <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;def-model.RDS&quot;</span>, <span class="st">&quot;validate-clean.txt&quot;</span>)</span>
<span id="cb4-3"><a href="#cb4-3" tabindex="-1"></a><span class="co"># The test environment is setup</span></span>
<span id="cb4-4"><a href="#cb4-4" tabindex="-1"></a>ed <span class="ot">&lt;-</span> <span class="fu">setup_env</span>(rf, ve)</span>
<span id="cb4-5"><a href="#cb4-5" tabindex="-1"></a></span>
<span id="cb4-6"><a href="#cb4-6" tabindex="-1"></a><span class="co"># The model file name</span></span>
<span id="cb4-7"><a href="#cb4-7" tabindex="-1"></a>mfn <span class="ot">&lt;-</span> <span class="fu">paste0</span>(ed, <span class="st">&quot;/def-model.RDS&quot;</span>)</span>
<span id="cb4-8"><a href="#cb4-8" tabindex="-1"></a><span class="co"># The path to the cleaned validation file</span></span>
<span id="cb4-9"><a href="#cb4-9" tabindex="-1"></a>vfn <span class="ot">&lt;-</span> <span class="fu">paste0</span>(ed, <span class="st">&quot;/validate-clean.txt&quot;</span>)</span>
<span id="cb4-10"><a href="#cb4-10" tabindex="-1"></a><span class="co"># ModelEvaluator class object is created</span></span>
<span id="cb4-11"><a href="#cb4-11" tabindex="-1"></a>me <span class="ot">&lt;-</span> ModelEvaluator<span class="sc">$</span><span class="fu">new</span>(<span class="at">mf =</span> mfn, <span class="at">ve =</span> ve)</span>
<span id="cb4-12"><a href="#cb4-12" tabindex="-1"></a><span class="co"># The intrinsic evaluation is performed on first 100 lines</span></span>
<span id="cb4-13"><a href="#cb4-13" tabindex="-1"></a>stats <span class="ot">&lt;-</span> me<span class="sc">$</span><span class="fu">extrinsic_evaluation</span>(<span class="at">lc =</span> <span class="dv">100</span>, <span class="at">fn =</span> vfn)</span>
<span id="cb4-14"><a href="#cb4-14" tabindex="-1"></a></span>
<span id="cb4-15"><a href="#cb4-15" tabindex="-1"></a><span class="co"># The test environment is cleaned up</span></span>
<span id="cb4-16"><a href="#cb4-16" tabindex="-1"></a><span class="fu">clean_up</span>(ve)</span></code></pre></div>
</div>
<div id="how-to-predict-a-word" class="section level2">
<h2>How to predict a word</h2>
<p>The following example shows how to predict the next word. It returns
the 3 possible next words along with their probabilities.</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" tabindex="-1"></a><span class="co"># The required files</span></span>
<span id="cb5-2"><a href="#cb5-2" tabindex="-1"></a>rf <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;def-model.RDS&quot;</span>, <span class="st">&quot;validate-clean.txt&quot;</span>)</span>
<span id="cb5-3"><a href="#cb5-3" tabindex="-1"></a><span class="co"># The test environment is setup</span></span>
<span id="cb5-4"><a href="#cb5-4" tabindex="-1"></a>ed <span class="ot">&lt;-</span> <span class="fu">setup_env</span>(rf, ve)</span>
<span id="cb5-5"><a href="#cb5-5" tabindex="-1"></a></span>
<span id="cb5-6"><a href="#cb5-6" tabindex="-1"></a><span class="co"># The model file name</span></span>
<span id="cb5-7"><a href="#cb5-7" tabindex="-1"></a>mfn <span class="ot">&lt;-</span> <span class="fu">paste0</span>(ed, <span class="st">&quot;/def-model.RDS&quot;</span>)</span>
<span id="cb5-8"><a href="#cb5-8" tabindex="-1"></a><span class="co"># An object of class ModelPredictor is created. The mf parameter is the name of</span></span>
<span id="cb5-9"><a href="#cb5-9" tabindex="-1"></a><span class="co"># the model file that was generated in the previous example.</span></span>
<span id="cb5-10"><a href="#cb5-10" tabindex="-1"></a>mp <span class="ot">&lt;-</span> ModelPredictor<span class="sc">$</span><span class="fu">new</span>(<span class="at">mf =</span> mfn, <span class="at">ve =</span> ve)</span>
<span id="cb5-11"><a href="#cb5-11" tabindex="-1"></a><span class="co"># Given the words: &quot;how are&quot;, the next word is predicted. The top 3 most likely</span></span>
<span id="cb5-12"><a href="#cb5-12" tabindex="-1"></a><span class="co"># next words are returned along with their respective probabilities.</span></span>
<span id="cb5-13"><a href="#cb5-13" tabindex="-1"></a>res <span class="ot">&lt;-</span> mp<span class="sc">$</span><span class="fu">predict_word</span>(<span class="at">words =</span> <span class="st">&quot;how are&quot;</span>, <span class="dv">3</span>)</span>
<span id="cb5-14"><a href="#cb5-14" tabindex="-1"></a><span class="co"># The test environment is cleaned up</span></span>
<span id="cb5-15"><a href="#cb5-15" tabindex="-1"></a><span class="fu">clean_up</span>(ve)</span></code></pre></div>
</div>
<div id="demo" class="section level2">
<h2>Demo</h2>
<p>The wordpredictor package includes a demo called “word-predictor”.
The demo is a Shiny application that displays the ten most likely words
for a given set of words. To access the demo, run the following command
from the R shell:</p>
<p><strong><code>demo(&quot;word-predictor&quot;, package = &quot;wordpredictor&quot;, ask = F)</code></strong>.</p>
</div>
<div id="package-dependencies" class="section level2">
<h2>Package dependencies</h2>
<p>The wordpredictor package uses the following packages: <a href="https://cran.r-project.org/package=digest">digest</a>, <a href="https://cran.r-project.org/package=dplyr">dply</a>, <a href="https://cran.r-project.org/package=ggplot2">ggplot2</a>, <a href="https://cran.r-project.org/package=R6">R6</a>, <a href="https://cran.r-project.org/package=testthat">testthat</a> and <a href="https://cran.r-project.org/package=stringr">stingr</a></p>
<p>The following packages were useful during package development: <a href="https://cran.r-project.org/package=quanteda">quanteda</a>, <a href="https://cran.r-project.org/package=tm">tm</a> and <a href="https://cran.r-project.org/package=hash">hash</a> <a href="https://cran.r-project.org/package=lintr">lintr</a> <a href="https://cran.r-project.org/package=styler">styler</a> <a href="https://cran.r-project.org/package=pkgdown">pkgdown</a> <a href="https://cran.r-project.org/package=pryr">pryr</a>,</p>
</div>
<div id="useful-links" class="section level2">
<h2>Useful Links</h2>
<p>The following articles and tutorials were very useful:</p>
<ul>
<li><a href="https://devopedia.org/n-gram-model">N-Gram Model</a></li>
<li><a href="https://lazyprogrammer.me/probability-smoothing-for-natural-language-processing/">Probability
Smoothing for Natural Language Processing</a></li>
<li><a href="https://medium.com/@ageitgey/natural-language-processing-is-fun-9a0bff37854e">Natural
Language Processing is Fun!</a></li>
<li><a href="https://tutorials.quanteda.io/">Quanteda Tutorials</a></li>
</ul>
</div>
<div id="bibliography" class="section level2 unnumbered">
<h2 class="unnumbered">Bibliography</h2>
<div id="refs" class="references csl-bib-body hanging-indent" entry-spacing="0">
<div id="ref-intro-to-natural-lang-proc" class="csl-entry">
Cardie, Professor Claire. 2014. <span>“Smoothing, Interpolation and
Backoff.”</span> In. <a href="https://www.cs.cornell.edu/courses/cs4740/2014sp/lectures/smoothing+backoff.pdf">https://www.cs.cornell.edu/courses/cs4740/2014sp/lectures/smoothing+backoff.pdf</a>.
</div>
<div id="ref-JSSv025i05" class="csl-entry">
Feinerer, Ingo, Kurt Hornik, and David Meyer. 2008. <span>“Text Mining
Infrastructure in r.”</span> <em>Journal of Statistical Software,
Articles</em> 25 (5): 1–54. <a href="https://doi.org/10.18637/jss.v025.i05">https://doi.org/10.18637/jss.v025.i05</a>.
</div>
<div id="ref-roxygen2-man" class="csl-entry">
Hadley Wickham, Gábor Csárdi, Peter Danenberg. 2020. <em>In-Line
Documentation for r</em>. <a href="https://cran.r-project.org/package=roxygen2">https://cran.r-project.org/package=roxygen2</a>.
</div>
<div id="ref-speech-and-lang-proc" class="csl-entry">
Jurafsky, Dan, and James H. Martin. 2020. <span>“N-Gram Language
Models.”</span> In <em>Speech and Language Processing</em>, third. <a href="https://web.archive.org/web/20240919222934/https%3A%2F%2Fweb.stanford.edu%2F~jurafsky%2Fslp3%2F3.pdf">https://web.archive.org/web/20240919222934/https%3A%2F%2Fweb.stanford.edu%2F~jurafsky%2Fslp3%2F3.pdf</a>.
</div>
<div id="ref-adv-r-man" class="csl-entry">
Wickham, Hadley. 2021. <em>Advanced r</em>. Second. <a href="https://adv-r.hadley.nz/index.html">https://adv-r.hadley.nz/index.html</a>.
</div>
<div id="ref-r-pkgs-man" class="csl-entry">
Wickham, Hadley, and Jenny Bryan. 2021. <em>R Packages</em>. Second. <a href="https://r-pkgs.org/index.html">https://r-pkgs.org/index.html</a>.
</div>
<div id="ref-enwiki:1018953040" class="csl-entry">
Wikipedia contributors. 2021a. <span>“N-Gram —
<span>Wikipedia</span><span>,</span> the Free Encyclopedia.”</span> <a href="https://en.wikipedia.org/w/index.php?title=N-gram&amp;oldid=1018953040">https://en.wikipedia.org/w/index.php?title=N-gram&amp;oldid=1018953040</a>.
</div>
<div id="ref-enwiki:1022965742" class="csl-entry">
———. 2021b. <span>“Perplexity — <span>Wikipedia</span><span>,</span> the
Free Encyclopedia.”</span> <a href="https://en.wikipedia.org/w/index.php?title=Perplexity&amp;oldid=1022965742">https://en.wikipedia.org/w/index.php?title=Perplexity&amp;oldid=1022965742</a>.
</div>
<div id="ref-r-markdown-man" class="csl-entry">
Yihui Xie, Emily Riederer, Christophe Dervieux. 2021. <em>R Markdown
Cookbook</em>. First. <a href="https://bookdown.org/yihui/rmarkdown-cookbook/">https://bookdown.org/yihui/rmarkdown-cookbook/</a>.
</div>
</div>
</div>



<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
